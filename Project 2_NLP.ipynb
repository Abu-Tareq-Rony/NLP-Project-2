{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e925b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4aad05-0177-4f7e-ad54-352195d73789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014aaadd-4119-4209-b6fe-801fa01c707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f2f5f6-1d81-41bc-9af6-dc5f714fd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304c50ef-2639-4af7-8888-c586355b677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q ipywidgets==8.1.2 jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037728cf-7488-491d-a889-c3cb76fbedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression  # kept import to preserve structure (not used)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Keep case for BioBERT (cased) while still cleaning text.\n",
    "    # If you ever switch back to uncased models, you can re-enable lowercasing.\n",
    "    # text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # keep case, strip non-letters\n",
    "    tokens = text.split()\n",
    "    # compare to stopwords in lowercase so we can preserve token case\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "with open(\"QTL_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['text'] = (df['Title'] + ' ' + df['Abstract']).apply(clean_text)\n",
    "df['label'] = df['Category'].astype(int)\n",
    "\n",
    "df_majority = df[df.label == 0]\n",
    "df_minority = df[df.label == 1]\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority]).sample(frac=1, random_state=42)\n",
    "\n",
    "X = df_balanced['text'].tolist()\n",
    "y = df_balanced['label'].tolist()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# TF-IDF + SVM (replacing Logistic Regression), then calibrate for probabilities\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LinearSVC(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 2), (1, 3)],\n",
    "    'vectorizer__max_df': [0.8, 0.9],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'vectorizer__max_features': [10000, 15000],\n",
    "    'clf__C': [0.1, 1, 10]  # SVM regularization strength\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, scoring='f1', cv=StratifiedKFold(n_splits=5), verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "calibrated_model = CalibratedClassifierCV(grid.best_estimator_, method='sigmoid', cv=5)\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# keep original variable name to preserve structure; this now holds SVM probabilities\n",
    "val_probs_lr = calibrated_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# BioBERT fine-tuning (swap from bert-base-uncased)\n",
    "bio_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bio_name)\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "val_df = pd.DataFrame({\"text\": X_val, \"label\": y_val})\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df).map(tokenize_fn, batched=True)\n",
    "val_ds = Dataset.from_pandas(val_df).map(tokenize_fn, batched=True)\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(bio_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": (pred.label_ids == preds).mean(),\n",
    "        \"f1\": f1_score(pred.label_ids, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "val_outputs_bert = trainer.predict(val_ds)\n",
    "val_probs_bert = softmax(val_outputs_bert.predictions, axis=1)[:, 1]\n",
    "\n",
    "# Ensemble (keep same structure/variable names; val_probs_lr now comes from calibrated SVM)\n",
    "f1_lr = f1_score(y_val, (val_probs_lr >= 0.5).astype(int))      # SVM F1 (name preserved)\n",
    "f1_bert = f1_score(y_val, (val_probs_bert >= 0.5).astype(int))\n",
    "w_lr = f1_lr / (f1_lr + f1_bert)\n",
    "w_bert = f1_bert / (f1_lr + f1_bert)\n",
    "\n",
    "ensemble_val_probs = (w_lr * val_probs_lr) + (w_bert * val_probs_bert)\n",
    "ensemble_val_preds = (ensemble_val_probs >= 0.5).astype(int)\n",
    "print(\"Validation F1 Score (Ensemble):\", f1_score(y_val, ensemble_val_preds, average=\"weighted\"))\n",
    "print(classification_report(y_val, ensemble_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad25bb2-14e6-436f-a7fb-7e58f7f5daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "test_df = pd.read_csv(\"QTL_test_unlabeled.tsv\", sep=\"\\t\")\n",
    "test_df['text'] = (test_df['Title'] + ' ' + test_df['Abstract']).apply(clean_text)\n",
    "X_test = test_df['text'].tolist()\n",
    "test_probs_lr = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_raw_df = pd.DataFrame({\"text\": X_test})\n",
    "test_ds = Dataset.from_pandas(test_raw_df).map(tokenize_fn, batched=True)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "test_outputs_bert = trainer.predict(test_ds)\n",
    "test_probs_bert = softmax(test_outputs_bert.predictions, axis=1)[:, 1]\n",
    "\n",
    "ensemble_test_probs = (w_lr * test_probs_lr) + (w_bert * test_probs_bert)\n",
    "ensemble_test_preds = (ensemble_test_probs >= 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PMID\": test_df[\"PMID\"],\n",
    "    \"Label\": ensemble_test_preds\n",
    "})\n",
    "submission.to_csv(\"Project 2_Submission_final.csv\", index=False)\n",
    "print(\"Project 2_Submission_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-env)",
   "language": "python",
   "name": "bert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
